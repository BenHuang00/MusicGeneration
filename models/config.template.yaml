LSTM:
  embedding_size: 256
  hidden_size: 512
  num_layers: 2
  bidirectional: True
  dropout: 0.5

Transformer:
  nhead: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  d_model: 512
  dim_feedforward: 2048
  dropout: 0.1

TransformerXL:
  n_layer: 10
  n_head: 8
  d_model: 1024
  d_head: 1024
  d_inner: 4096
  dropout: 0.1
  dropatt: 0.0
  tie_weight: True
  d_embed: 512
  div_val: 1
  tie_projs: [False]
  pre_lnorm: True
  tgt_len: 70
  ext_len: 0
  mem_len: 512
  cutoffs: []
  adapt_cutoffs: False
  same_length: True
  attn_type: 0
  clamp_len: -1
  sample_softmax: -1

Mamba:
  d_model: 128
  d_state: 64
  d_conv: 8
  expand: 2
  dropout: 0.1